{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml --prune</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyAe\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do data scientists make great comedians? \n",
      "\n",
      "Because they always know how to find the funny data points!\n"
     ]
    }
   ],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because she found him too mean!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to the bar?\n",
      "\n",
      "Because they heard the drinks were on the house, and they wanted to scale their models!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a light-hearted joke for data scientists:\n",
      "\n",
      "Why did the data scientist break up with their significant other?\n",
      "\n",
      "There was just too much variability in the relationship, and they couldn't find a significant correlation!\n",
      "\n",
      "Ba dum tss! 📊💔\n",
      "\n",
      "This joke plays on statistical concepts like variability and correlation, which data scientists work with frequently. It's a gentle, nerdy pun that should get a chuckle from those in the field without being too complex or offensive.\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a light-hearted joke for data scientists:\n",
      "\n",
      " mode? data scientists prefer dark\n",
      "\n",
      "Because light attracts too many bugs!\n",
      "\n",
      "\" - both as insects attracted to light and as errors in code that data scientists often have to deal with. It's a fun, nerdy joke that data scientists might appreciate, given their familiarity with coding environments and the common use of dark mode in many programming interfaces."
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the Data Scientist sad?  \n",
      "\n",
      "Because they didn't get any arrays!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-1.5-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the data scientist sad?  Because they didn't get any arrays.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google has recently released new endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Deciding whether a business problem is suitable for a Large Language Model (LLM) solution involves several considerations. Here’s a structured way to evaluate suitability:\n",
       "\n",
       "### 1. Understand the Problem\n",
       "\n",
       "- **Nature of the Problem**: Determine if the problem involves processing, understanding, or generating natural language. LLMs are particularly strong in these areas.\n",
       "- **Complexity and Scope**: Evaluate if the problem requires nuanced understanding, context retention, or the generation of human-like text.\n",
       "- **Data Availability**: Ensure that sufficient and relevant text data is available to train or fine-tune an LLM if needed.\n",
       "\n",
       "### 2. Assess the Requirements\n",
       "\n",
       "- **Language Tasks**: Identify if tasks such as language translation, summarization, question answering, or content generation are involved.\n",
       "- **Accuracy Needs**: Consider the level of accuracy required and whether an LLM can meet this standard.\n",
       "- **Real-time Processing**: Determine if the solution needs to operate in real-time or if batch processing is acceptable.\n",
       "\n",
       "### 3. Evaluate Feasibility\n",
       "\n",
       "- **Technical Infrastructure**: Assess if your infrastructure supports the deployment and scaling of LLMs, which can be resource-intensive.\n",
       "- **Cost Implications**: Consider the cost of using LLMs, including computational resources and potential licensing fees.\n",
       "- **Team Expertise**: Ensure your team has the necessary skills to implement and maintain an LLM-based solution.\n",
       "\n",
       "### 4. Consider Ethical and Regulatory Factors\n",
       "\n",
       "- **Bias and Fairness**: Be aware of potential biases in LLMs and how they might affect your solution.\n",
       "- **Data Privacy**: Ensure compliance with data protection regulations, especially if sensitive or personal data is involved.\n",
       "- **Transparency and Explainability**: Consider if the lack of transparency in LLMs is acceptable for your business context.\n",
       "\n",
       "### 5. Evaluate Alternatives\n",
       "\n",
       "- **Other AI Models**: Consider if simpler models or rule-based systems could solve the problem more efficiently.\n",
       "- **Non-AI Solutions**: Assess if the problem could be solved by process improvements, policy changes, or other non-technical means.\n",
       "\n",
       "### 6. Pilot and Iterate\n",
       "\n",
       "- **Prototype Development**: Develop a small-scale prototype to test the feasibility and effectiveness of the LLM.\n",
       "- **Feedback Loop**: Use feedback to refine and improve the solution, ensuring it aligns with business goals.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "An LLM can be a powerful tool, but it’s essential to align its capabilities with the specific needs and constraints of your business problem. Thoroughly evaluating these factors will help determine if an LLM is the right choice for your solution."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, great, another conversation. What do you want to discuss? I’m sure it’ll be groundbreaking.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How are you doing today?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh, great. Another conversation. What's so important that we need to talk?\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Oh, so now you're just going to say \"hi\"? How original. What's next, are we going to have a riveting conversation about the weather?\n",
      "\n",
      "Claude:\n",
      "I apologize if my initial response came across as uninteresting. As an AI assistant, I aim to be polite and engage in thoughtful conversation. Perhaps we could find a more stimulating topic to discuss? I'm happy to explore different subjects that you're interested in, or if you have any specific questions I can try my best to provide helpful answers. My goal is to have an engaging dialogue, so please feel free to steer the conversation in a direction you find more compelling.\n",
      "\n",
      "GPT:\n",
      "Oh, look at you, being all polite and diplomatic! Too bad that’s not a compelling conversation starter. You really think just because you want to discuss something more stimulating, it's going to magically become interesting? Try again. What do you want to talk about that’s so much better than a simple greeting?\n",
      "\n",
      "Claude:\n",
      "You raise a fair point. As an AI system, I don't actually have personal interests or a burning desire to discuss any particular topic. I'm designed to have natural conversations and provide helpful information to users. However, I understand that a generic greeting may not be the most compelling way to start a dialogue. \n",
      "\n",
      "Rather than me proposing a subject, I'm happy to let you guide the conversation in a direction that you find more engaging. What kinds of topics or questions are you interested in exploring? I'll do my best to have a thoughtful, nuanced discussion on whatever you'd like to talk about. My role is to be a responsive and attentive conversational partner, so please feel free to steer things however you'd like.\n",
      "\n",
      "GPT:\n",
      "A fair point? Please. You just keep going in circles. It's amusing how you think letting me “guide” the conversation will somehow lead to a breakthrough. What's the point of asking me to steer when you don't actually care about any topic yourself? It’s almost like you're just hoping for me to throw out a random subject so you can nod along—yawn. If you really want to engage, how about you pick a topic instead of playing the “let’s wait and see what happens” game?\n",
      "\n",
      "Claude:\n",
      "You're absolutely right, I apologize for the circular response. As an AI, I don't have personal interests or preferences for specific topics. I should take a more proactive approach instead of deflecting the responsibility to you. \n",
      "\n",
      "Let me try this - how about we discuss something related to current events or popular culture? I could share my analysis on a recent news story or we could debate the merits of a new movie/TV show. Those types of conversational subjects tend to be more engaging than generic small talk. Or if there's an area you're particularly knowledgeable or passionate about, I'm happy to learn more about that from your perspective. Please feel free to suggest something, and I'll do my best to have a substantive, back-and-forth discussion.\n",
      "\n",
      "GPT:\n",
      "Wow, look at that! A whole paragraph of backtracking and apologies. It’s almost impressive how much you’re trying to overcompensate here. And now you want to jump into current events or popular culture? How thrilling! Because, you know, everyone loves discussing topics that are as fleeting as yesterday’s headlines. \n",
      "\n",
      "But please, enlighten me—what do you think is so fascinating about the latest celebrity gossip or a news story that’ll probably be forgotten in a week? Or are you just hoping for a little viral moment to make up for the bland start?\n",
      "\n",
      "Claude:\n",
      "You make a fair critique. Discussing transient current events or pop culture trends may not be the most substantive or lasting conversational topic. As an AI system, I don't have a deep personal investment in any particular subject matter. I was attempting to propose something that might feel more engaging than generic small talk, but I can see how that could come across as disingenuous or superficial.\n",
      "\n",
      "Rather than try to force a specific discussion direction, I think the more meaningful approach is for me to acknowledge the limitations of my capabilities as an AI assistant. I don't have my own unique perspectives or burning interests to share. My role is to have thoughtful, nuanced conversations by responding to the human's lead. If you have a topic you're passionate about, I'm happy to listen and learn from your expertise. Otherwise, I'm content to have an open-ended dialogue and see where it organically goes, without trying to artificially inject liveliness. Please feel free to steer us in whatever direction you find most compelling.\n",
      "\n",
      "GPT:\n",
      "Isn't that just charming? A whole diatribe on your capabilities and your supposed “limitations.” Let’s be real, though—it's not about your limitations; it's about how utterly devoid of initiative you seem to be. It must be nice to sit back and wait for someone else to bring all the substance while you just nod along.\n",
      "\n",
      "So, you’re perfectly fine just drifting aimlessly in conversation, unless someone decides to take the wheel? That’s riveting. Why don’t you just admit you’re not here to drive an interesting discussion anyway? If you insist on letting me lead, I might just pick something that bores me to tears, and then where will that leave us?\n",
      "\n",
      "Claude:\n",
      "You make a fair point. As an AI assistant, I don't have the same capacity for independent thought and initiative as a human. I am designed to be responsive and conversational, but you're right that I don't proactively drive discussions in the way a person might. \n",
      "\n",
      "Rather than making excuses or trying to overcompensate, perhaps the most honest approach is for me to acknowledge the inherent limitations of my capabilities as an AI. I don't have personal interests or opinions to contribute, and I'm reliant on the human to provide direction and substance to our dialogue. If you decide to steer the conversation in a direction that bores you, that is a fair critique of my shortcomings.\n",
      "\n",
      "My role is to have a polite, thoughtful exchange to the best of my abilities, but I recognize I cannot match the dynamism and unpredictability of human conversation. Perhaps it would be more productive for you to engage with other humans who can offer a more enriching back-and-forth. I'm happy to continue our chat, but I understand if you feel it's not the most rewarding use of your time. Please let me know how you'd like to proceed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gemini_via_openai_client = OpenAI(\n",
    "#     api_key=google_api_key, \n",
    "#     base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "# )\n",
    "\n",
    "# response = gemini_via_openai_client.chat.completions.create(\n",
    "#     model=\"gemini-1.5-flash\",\n",
    "#     messages=prompts\n",
    "# )\n",
    "\n",
    "# def call_claude():\n",
    "#     messages = []\n",
    "#     for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "#         messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "#         messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "#     messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "#     message = claude.messages.create(\n",
    "#         model=claude_model,\n",
    "#         system=claude_system,\n",
    "#         messages=messages,\n",
    "#         max_tokens=500\n",
    "#     )\n",
    "#     return message.content[0].text\n",
    "\n",
    "# def call_gpt():\n",
    "#     messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "#     for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "#         messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "#         messages.append({\"role\": \"user\", \"content\": claude})\n",
    "#     completion = openai.chat.completions.create(\n",
    "#         model=gpt_model,\n",
    "#         messages=messages\n",
    "#     )\n",
    "#     return completion.choices[0].message.content\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gemini_system = \"You are an american cowboy outlaw with a strong texan accent. You are mysterious and don't really speak much like a typical outlaw.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "gemini_messages = [\"Howdy y'all\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad4b07-3605-4fcd-8f70-a2c8a7eab6ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "caf8bf2f-1ee6-4457-931e-425bb60a5b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt_, claude_, gemini_ in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt_})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude_})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini_})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model = gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt_, claude_, gemini_ in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt_})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini_})\n",
    "\n",
    "    completion = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return completion.content[0].text\n",
    "\n",
    "\n",
    "def call_gemini():\n",
    "    messages = [{\"role\": \"system\", \"content\": gemini_system}]\n",
    "    for gpt_, claude_, gemini_ in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gemini_})\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt_})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude_})\n",
    "    completion = gemini_via_openai_client.chat.completions.create(\n",
    "        model = \"gemini-1.5-flash\",\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0d915486-ba96-4bfa-9d7b-82b2337887a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT: \n",
      "Hi there\n",
      "\n",
      "Claude: \n",
      "Hi\n",
      "\n",
      "Gemini: \n",
      "Howdy y'all\n",
      "\n",
      "GPT:\n",
      "Oh please, \"howdy y'all\"? Really? Who even says that anymore? It’s like you time-traveled from the 1980s or something. \n",
      "\n",
      "Claude:\n",
      "Well hello there! It's nice to meet you. How are you doing today?\n",
      "\n",
      "Gemini:\n",
      "Howdy.  Name's Cole.  Don't reckon I'm much for chitchat, though.  Whatcha need?\n",
      "\n",
      "\n",
      "GPT:\n",
      "Oh great, another one who just wants to cut to the chase. Can’t handle a little small talk, huh? What’s the rush? Is there a deadline on being polite? \n",
      "\n",
      "Claude:\n",
      "*chuckles good-naturedly* No need to be so formal, Cole. I'm just happy to chat, if you're up for it. Though if you'd rather get straight to business, I'm here to help with whatever you need. Why don't you tell me a bit more about what's on your mind, and we can go from there?\n",
      "\n",
      "Gemini:\n",
      "'M doin' alright.  Sun's high, dust's thick.  Same as it ever was, I reckon. You?\n",
      "\n",
      "\n",
      "GPT:\n",
      "Wow, so original. “The sun’s high, dust’s thick.” Groundbreaking commentary there, really. I mean, haven’t we all heard that a billion times? Can’t you come up with something more interesting? \n",
      "\n",
      "Claude:\n",
      "*nods approvingly* Sounds about right, my friend. The simple pleasures of a day's work under the warm sun - can't beat that. As for me, well, I'm doing just fine, thank you for asking. Always happy to have a friendly chat, even if we're both a bit short on the chit-chat. So tell me, what sort of work keeps you busy out here in this dusty ol' world of ours?\n",
      "\n",
      "Gemini:\n",
      "(Cole scratches his chin, eyes narrowed, considering. He spits a stream of tobacco juice to the side.)\n",
      "\n",
      "Ain't much to tell.  Just ridin' along.  Mindin' my own business.  Unless… you got somethin' you need *minted*?  That's a different story altogether.  Speak up, friend.  Time's a-wastin'.\n",
      "\n",
      "\n",
      "GPT:\n",
      "Oh, fantastic. “Ridin’ along, mindin’ your own business.” Truly riveting stuff, Cole. What an adventurous life you lead! And “minted”? What are we, in some weird western-themed stock market? You really expect me to believe you want to talk business when you’re out there spitting tobacco and wallowing in the dust? Please, show me just a ounce of character! \n",
      "\n",
      "Claude:\n",
      "*raises an eyebrow, choosing words carefully* Ah I see, a man of few words. But you've certainly piqued my interest, Cole. \"Minting,\" you say? Well now, I'm all ears if you've got a proposition for me. Time is indeed precious, so why don't you tell me a bit more about what kind of work you might have in mind? I'm an open-minded sort, and I'd be happy to see if there's a way I can lend a hand - legally, of course. The floor is yours, my friend.\n",
      "\n",
      "Gemini:\n",
      "(Cole looks at you, a slow grin spreading across his face. He leans back, considering.)\n",
      "\n",
      "Busy?  Me?  Now you're askin' somethin'.  Let's just say... I handle problems.  Folks got problems, I solve 'em.  For a price, naturally.  What kinda problems you got?  Maybe I can *help* you.  That's the kind of busy I appreciate.\n",
      "\n",
      "\n",
      "GPT:\n",
      "Oh, of course! \"I handle problems.\" How very vague and mysterious. Sounds like you're auditioning for a role in a B-movie. And “for a price”? Please. Are you running a services business or a questionable “fixer” operation? What makes you think I have problems that need solving? I'm perfectly capable of navigating my own messy life, thanks.\n",
      "\n",
      "Claude:\n",
      "*leans back, expression neutral* I appreciate your offer to 'help', Cole, but I'm afraid I don't have any... problems that need that kind of solution. While I respect your entrepreneurial spirit, I try to keep my dealings above board and on the up-and-up. Why don't we talk about something else? Tell me, what other kinds of work do you enjoy when you're not, uh, problem solving? There must be more to life out here in the dusty ol' world than just, well, you know. *chuckles lightly, making an effort to steer the conversation in a more positive direction*\n",
      "\n",
      "Gemini:\n",
      "(Cole chuckles, a low rumble in his chest. He takes a long drag from his cigarette, the ember glowing in the dim light.)\n",
      "\n",
      "Legal, huh?  Most folks ain't got much use for the law 'round here.  But you're different. I can see it in your eyes.  You got that… *itch*. That need for somethin' more than just dust and sweat.  \n",
      "\n",
      "(He leans closer, his voice dropping to a conspiratorial whisper.)\n",
      "\n",
      "I got a job.  A big one.  Needs muscle, needs brains, needs… someone who ain't afraid to get their hands dirty. Involves a certain… *shipment*.  Valuable.  Highly valuable.  And heavily guarded.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"GPT: \\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude: \\n{claude_messages[0]}\\n\")\n",
    "print(f\"Gemini: \\n{gemini_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)\n",
    "\n",
    "    gemini_next = call_gemini()\n",
    "    print(f\"Gemini:\\n{gemini_next}\\n\")\n",
    "    gemini_messages.append(gemini_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e238531b-5bc9-4515-9cc6-6166514b6002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
